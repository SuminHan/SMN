{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named theano",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-43b8bfcf95ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named theano"
     ]
    }
   ],
   "source": [
    "import theano, cPickle, h5py, lasagne, random, csv, gzip, time                                                  \n",
    "import numpy as np\n",
    "import theano.tensor as T \n",
    "from layers import *\n",
    "from util import *        \n",
    "\n",
    "# assemble the network\n",
    "def build_rmn(d_word, d_char, d_book, d_hidden, len_voc, \n",
    "    num_descs, num_chars, num_books, span_size, We, \n",
    "    freeze_words=True, eps=1e-5, lr=0.01, negs=10):\n",
    "\n",
    "    # input theano vars\n",
    "    in_spans = T.imatrix(name='spans')\n",
    "    in_neg = T.imatrix(name='neg_spans')\n",
    "    in_chars = T.ivector(name='chars')\n",
    "    in_book = T.ivector(name='books')\n",
    "    in_currmasks = T.matrix(name='curr_masks')\n",
    "    in_dropmasks = T.matrix(name='drop_masks')\n",
    "    in_negmasks = T.matrix(name='neg_masks')\n",
    "\n",
    "    # define network\n",
    "    l_inspans = lasagne.layers.InputLayer(shape=(None, span_size), \n",
    "        input_var=in_spans)\n",
    "    l_inneg = lasagne.layers.InputLayer(shape=(negs, span_size), \n",
    "        input_var=in_neg)\n",
    "    l_inchars = lasagne.layers.InputLayer(shape=(2, ), \n",
    "        input_var=in_chars)\n",
    "    l_inbook = lasagne.layers.InputLayer(shape=(1, ), \n",
    "        input_var=in_book)\n",
    "    l_currmask = lasagne.layers.InputLayer(shape=(None, span_size), \n",
    "        input_var=in_currmasks)\n",
    "    l_dropmask = lasagne.layers.InputLayer(shape=(None, span_size), \n",
    "        input_var=in_dropmasks)\n",
    "    l_negmask = lasagne.layers.InputLayer(shape=(negs, span_size), \n",
    "        input_var=in_negmasks)\n",
    "\n",
    "    # negative examples should use same embedding matrix\n",
    "    l_emb = MyEmbeddingLayer(l_inspans, len_voc, \n",
    "        d_word, W=We, name='word_emb')\n",
    "    l_negemb = MyEmbeddingLayer(l_inneg, len_voc, \n",
    "            d_word, W=l_emb.W, name='word_emb_copy1')\n",
    "\n",
    "    # freeze embeddings\n",
    "    if freeze_words:\n",
    "        l_emb.params[l_emb.W].remove('trainable')\n",
    "        l_negemb.params[l_negemb.W].remove('trainable')\n",
    "\n",
    "    l_chars = lasagne.layers.EmbeddingLayer(\\\n",
    "        l_inchars, num_chars, d_char, name='char_emb')\n",
    "    l_books = lasagne.layers.EmbeddingLayer(\\\n",
    "        l_inbook, num_books, d_book, name='book_emb')\n",
    "\n",
    "    # average each span's embeddings\n",
    "    l_currsum = AverageLayer([l_emb, l_currmask], d_word)\n",
    "    l_dropsum = AverageLayer([l_emb, l_dropmask], d_word)\n",
    "    l_negsum = AverageLayer([l_negemb, l_negmask], d_word)\n",
    "\n",
    "    # pass all embeddings thru feed-forward layer\n",
    "    l_mix = MixingLayer([l_dropsum, l_chars, l_books],\n",
    "        d_word, d_char, d_book)\n",
    "\n",
    "    # compute recurrent weights over dictionary\n",
    "    l_rels = RecurrentRelationshipLayer(\\\n",
    "        l_mix, d_word, d_hidden, num_descs)\n",
    "\n",
    "    # multiply weights with dictionary matrix\n",
    "    l_recon = ReconLayer(l_rels, d_word, num_descs)\n",
    "\n",
    "    # compute loss\n",
    "    currsums = lasagne.layers.get_output(l_currsum)\n",
    "    negsums = lasagne.layers.get_output(l_negsum)\n",
    "    recon = lasagne.layers.get_output(l_recon)\n",
    "\n",
    "    currsums /= currsums.norm(2, axis=1)[:, None]\n",
    "    recon /= recon.norm(2, axis=1)[:, None]\n",
    "    negsums /= negsums.norm(2, axis=1)[:, None]\n",
    "    correct = T.sum(recon * currsums, axis=1)\n",
    "    negs = T.dot(recon, negsums.T)\n",
    "    loss = T.sum(T.maximum(0., \n",
    "        T.sum(1. - correct[:, None] + negs, axis=1)))\n",
    "\n",
    "    # enforce orthogonality constraint\n",
    "    norm_R = l_recon.R / l_recon.R.norm(2, axis=1)[:, None]\n",
    "    ortho_penalty = eps * T.sum((T.dot(norm_R, norm_R.T) - \\\n",
    "        T.eye(norm_R.shape[0])) ** 2)\n",
    "    loss += ortho_penalty\n",
    "\n",
    "    all_params = lasagne.layers.get_all_params(l_recon, trainable=True)\n",
    "    updates = lasagne.updates.adam(loss, all_params, learning_rate=lr)\n",
    "    traj_fn = theano.function([in_chars, in_book, \n",
    "        in_spans, in_dropmasks], \n",
    "        lasagne.layers.get_output(l_rels))\n",
    "    train_fn = theano.function([in_chars, in_book, \n",
    "        in_spans, in_currmasks, in_dropmasks,\n",
    "        in_neg, in_negmasks], \n",
    "        [loss, ortho_penalty], updates=updates)\n",
    "    return train_fn, traj_fn, l_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9e6a5cda4501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'loading data...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspan_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbmap\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/relationships.csv.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/metadata.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mWe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/glove.We'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnorm_We\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWe\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mWe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_We\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "print 'loading data...'\n",
    "span_data, span_size, wmap, cmap, bmap = \\\n",
    "    load_data('data/relationships.csv.gz', 'data/metadata.pkl')\n",
    "We = cPickle.load(open('data/glove.We', 'rb')).astype('float32')\n",
    "norm_We = We / np.linalg.norm(We, axis=1)[:, None]\n",
    "We = np.nan_to_num(norm_We)\n",
    "descriptor_log = 'models/descriptors.log'\n",
    "trajectory_log = 'models/trajectories.log'\n",
    "\n",
    "# embedding/hidden dimensionality\n",
    "d_word = We.shape[1]\n",
    "d_char = 50\n",
    "d_book = 50\n",
    "d_hidden = 50\n",
    "\n",
    "# number of descriptors\n",
    "num_descs = 30\n",
    "\n",
    "# number of negative samples per relationship\n",
    "num_negs = 50\n",
    "\n",
    "# word dropout probability\n",
    "p_drop = 0.75\n",
    "\n",
    "n_epochs = 15\n",
    "lr = 0.001\n",
    "eps = 1e-6\n",
    "num_chars = len(cmap)\n",
    "num_books = len(bmap)\n",
    "num_traj = len(span_data)\n",
    "len_voc = len(wmap)\n",
    "revmap = {}\n",
    "for w in wmap:\n",
    "    revmap[wmap[w]] = w\n",
    "\n",
    "print d_word, span_size, num_descs, len_voc,\\\n",
    "    num_chars, num_books, num_traj\n",
    "\n",
    "print 'compiling...'\n",
    "train_fn, traj_fn, final_layer = build_rmn(\n",
    "    d_word, d_char, d_book, d_hidden, len_voc, num_descs, num_chars, \n",
    "    num_books, span_size, We, eps=eps, \n",
    "    freeze_words=True, lr=lr, negs=num_negs)\n",
    "print 'done compiling, now training...'\n",
    "\n",
    "# training loop\n",
    "min_cost = float('inf')\n",
    "for epoch in range(n_epochs):\n",
    "    cost = 0.\n",
    "    random.shuffle(span_data)\n",
    "    start_time = time.time()\n",
    "    for book, chars, curr, cm, in span_data:\n",
    "        ns, nm = generate_negative_samples(\\\n",
    "            num_traj, span_size, num_negs, span_data)\n",
    "\n",
    "        # word dropout\n",
    "        drop_mask = (np.random.rand(*(cm.shape)) < (1 - p_drop)).astype('float32')\n",
    "        drop_mask *= cm\n",
    "\n",
    "        ex_cost, ex_ortho = train_fn(chars, book, curr, cm, drop_mask,\n",
    "            ns, nm)\n",
    "        cost += ex_cost\n",
    "    end_time = time.time()\n",
    "\n",
    "    # save params if cost went down\n",
    "    if cost < min_cost:\n",
    "        min_cost = cost\n",
    "        params = lasagne.layers.get_all_params(final_layer)\n",
    "        p_values = [p.get_value() for p in params]\n",
    "        p_dict = dict(zip([str(p) for p in params], p_values))\n",
    "        cPickle.dump(p_dict, open('models/rmn_params.pkl', 'wb'),\n",
    "            protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # compute nearest neighbors of descriptors\n",
    "        R = p_dict['R']\n",
    "        log = open(descriptor_log, 'w')\n",
    "        for ind in range(len(R)):\n",
    "            desc = R[ind] / np.linalg.norm(R[ind])\n",
    "            sims = We.dot(desc.T)\n",
    "            ordered_words = np.argsort(sims)[::-1]\n",
    "            desc_list = [ revmap[w] for w in ordered_words[:10]]\n",
    "            log.write(' '.join(desc_list) + '\\n')\n",
    "            print 'descriptor %d:' % ind\n",
    "            print desc_list\n",
    "        log.flush()\n",
    "        log.close()\n",
    "\n",
    "        # save relationship trajectories\n",
    "        print 'writing trajectories...'\n",
    "        tlog = open(trajectory_log, 'wb')\n",
    "        traj_writer = csv.writer(tlog)\n",
    "        traj_writer.writerow(['Book', 'Char 1', 'Char 2', 'Span ID'] + \\\n",
    "            ['Topic ' + str(i) for i in range(num_descs)])\n",
    "        for book, chars, curr, cm in span_data:\n",
    "            c1, c2 = [cmap[c] for c in chars]\n",
    "            bname = bmap[book[0]]\n",
    "\n",
    "            # feed unmasked inputs to get trajectories\n",
    "            traj = traj_fn(chars, book, curr, cm)\n",
    "            for ind in range(len(traj)):\n",
    "                step = traj[ind]\n",
    "                traj_writer.writerow([bname, c1, c2, ind] + \\\n",
    "                list(step) )   \n",
    "\n",
    "        tlog.flush()\n",
    "        tlog.close()\n",
    "\n",
    "    print 'done with epoch: ', epoch, ' cost =',\\\n",
    "        cost / len(span_data), 'time: ', end_time-start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
